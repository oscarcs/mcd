# Chapter 2
## Program Text to Tokens - Lexical Analysis

The front-end of a compiler starts with a stream of characters which constitute the program text, and creates from this stream intermediate code that allows context handling and translation into target code. It does this by obtaining the syntactic structure of the code by parsing the program according to the grammar of the language.

The syntactic analysis of the program produces a syntax tree, which usually contains too much useless information, and so a cleaned up version of the syntax tree, called the abstract syntax tree (AST) is usually used instead. The distinction between the syntax tree and the AST is a practical one, and the decision about what to include in the AST is left to the compiler engineer.

An alternative to ASTs is described in Waddle, 1990. A *production tree* is a tree of production nodes; the grammar symbols have no representation apart from their position in a production's RHS. A production tree can be represented with the following data structure:

``` C
typedef struct ProdTreeNode {
    int ProdNo;             // The index of the production.
    ProdTreeNode* Parent;   // Pointer to the parent node in the tree.
    ParsePtr Child[1];      // Variable-length array of pointers to node's child (rhs) nodes.
} ProdTreeNode;
```

### 2.1 Reading the program text

Reading and lexing the program text can be one of the more time-consuming parts of the compiler, so the textbook focusses more on efficiency here than it does in other sections.

The first thing to note is that on comtemporary systems with large amounts of memory, loading the entire file into memory at once is generally advisable. This reduces the complexity of the lexer, because we don't have to factor in the boundaries between tokens when reading in the file. We can also use the file to report context in error messages. 

Newlines are one particular annoyance when reading in files to the lexer. Each operating system has a different convention about what a newline is. In UNIX-based systems, the newline convention is a single character, LF, with value 0x0A ('\n'). On DOS-based systems, OS/2, Symbian, Palm OS, and others, the convention is two characters, CR LF, with value 0x0D 0x0A ('\r\n'). On Commodore, Acorn, ZX Specturum, Apple II and classic Mac OS systems, the convention is simply CR. Therefore, it is a good idea to convert all line ending characters to a fixed internal format as soon as possible.

### 2.2 Lexical versus syntactic analysis

The line between lexical and syntactic analysis is not as obvious as it seems. What is a token? Given an assignment operator `:=`, if the `:` and the `=` can stand apart from each other, then they are separate tokens. If they can't, then they are a single token. Comments and whitespace are generally not tokens (except in, say, Python), but it may be useful to preserve them, in order to show the program text surrounding an error.

### 2.3 Regular expressions and regular descriptions

*Regular expressions* are one way of describing tokens formally. A regular expression is a formula that describes a possibly infinite set of strings, in much the same way as a grammar. When we have a string that can be generated by a particular regular expression, we say that the regular expression *matches* the string.

| Basic Pattern | Matching string                           |
|---------------|-------------------------------------------|
| x             | The character x                           |
| .             | Any character (usually except a newline)  |
| R?            | 0 or 1 Rs.                                |
| R*            | 0 or more Rs                              |
| R+            | 1 or more Rs                              |
| RS            | R followed by S                           |
| R\|S          | R or S                                    |
| (R)           | R itself (group)                          |

The most basic regular expressions are a single character which matches just that character, e.g. the pattern `a` which just matches the string 'a'.

Basic patterns can be followed with repetition and composition operators. Parentheses can be used for grouping. The expression `ab*|cd?` is equivalent to `(a(b*))|(c(d?))`.

To match charactes like |, ?, *, and + that are part of the regular expression itself, generally we use escape characters, which are the characters prefixed with a `\`.

### 2.4 Lexical Analysis

Each token in the source language can therefore be sepcified in terms of a regular expression (or 'regular description'). The combination of a regular expression and the token name is called the *token description*. The task of the lexical analyser is to determine which of the regular expressions in S will match a segment of the input starting a P, where S is a set of token descriptions, and P is a position in the input stream. If there is more than one possible match, the lexer needs a disambiguating rule. Typically, the longest match is chosen; this is known as the *maximal-match rule*.

### 2.5 Creating a lexical analyzer by hand

Lexical analysers can be written by hand or generated automatically based on the specification of tokens through regular expressions. It is pretty easy to write a lexer by hand. Probably the best way to start is with a case statement over the first character of the input. The first characters of the tokens are different, and a case statement will subdivide the lexical analysis into smaller subproblems.

For different types of tokens, often the input will be from a finite set and the result will depend on the input only, which is a prime opportunity for *precomputation*. We compute all the answers in advance and then store them in an array, which means we can check the answers using only an array lookup.

### 2.6 Creating a lexical analyzer automatically

We can also generate a lexical analyzer.